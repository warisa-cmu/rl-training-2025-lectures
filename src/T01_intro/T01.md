---
marp: true
theme: default
class:
  - invert
author: Nirand Pisutha-Arnond
paginate: true
footer: "RL Training 2025"
math: mathjax
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Prompt:ital,wght@0,100;0,300;0,400;0,700;1,100;1,300;1,400;1,700&display=swap');

    :root {
    font-family: Prompt;
    --hl-color: #D57E7E;
}
h1 {
  font-family: Prompt
}
</style>

# Reinforcement Learning Training 2025

> Round 1

---

# Where is RL in ML?

![bg contain right:70%](img/paste-1752209927329.png)

---

# Supervised learning

- We know _all_ the right answers (label)
- We teach machine.

---

# Unsupervised learning

- We don't know the answer.
- We let machine find structure in the data.

---

# Reinforcement learning

- We don't know _all_ the right answer
  - but we have a way to conduct _trial-and-error_ experiments.
- We let the machine _discover_ the answers.

---

# Applications

- `ChatGPT`
  - Enhanced by reinforcement learning through a technique called Reinforcement Learning from Human Feedback (RLHF). [[1]](https://youtu.be/WMmGzx-jWvs?si=w6SjqgyBfdlpla6D) [[2]](https://huggingface.co/blog/rlhf)
- `Spot`
  - Utilize reinforcement learning (RL) to enhance their locomotion and manipulation capabilities. [[3]](https://bostondynamics.com/video/reinforcement-learning-with-spot)

---

# Types of RL

- Don't worry. We will come back later.
  ![bg contain right:60%](img/paste-1752284885846.png)

---

# RL formalism

- Entities
  - Agent
  - Environment
- Communimation
  - Actions
  - Reward
  - Observation

![bg contain right:60%](img/paste-1752285022155.png)

---

# Reward

- A scalar value we obtain periodically from the environment.
  - Can be positive or negative
- Tell our agent how well it has behaved.
- Reflects the success of the agent's recent activity (local)
  - Not all the successes achieved by the agent so far.
- What an agent is trying to achieve is the largest _accumulated_ reward over its sequence of actions.

---

# Agent

- An agent is somebody or something who/that interacts with the environment by executing certain actions, making observations, and receiving eventual rewards for this.
- In most practical RL scenarios, the agent is our piece of software that is supposed to solve some problem in a more-or-less efficient way.

---

# Environment

- The environment is everything outside of an agent.
- The agent's communication with the environment is limited to
  - Reward (obtained from the environment)
  - Actions (executed by the agent and given to the environment)
  - Observations (some information besides the reward that the agent receives from the environment).

---

# Action

- Actions are things that an agent can do in the environment.
- We distinguish between two types of actions—discrete or continuous.
  - **Discrete actions** form the finite set of mutually exclusive things an agent can do, such as move left or right.
  - **Continuous actions** have some value attached to them, such as a car's action turn the wheel having an angle and direction of steering.

---

# Observation

- Observations are pieces of information that the environment provides the agent with that say what's going on around the agent.
- _I am guessing it is something that agent can use to make action?_

---

# Markov processes (MP)

- Also called a Markov chain
- Models a system observed through a sequence of states.
- The system transitions between states according to certain dynamics, but the observer cannot influence the system.

---

# MP - State space

- The set of all possible states is called the state space.
- For MPs, the state space is finite but can be very large.
- Observations form a sequence or chain of states, known as the history.

---

# MP - Markov property

- The future state depends only on the current state, not on the full history.
- Each state is self-contained and unique.
- This simplifies modeling by focusing only on the current state to predict the future.

---

# MP - Example (weather model)

- States: `{sunny, rainy}`
- Sequence example: `[sunny, sunny, rainy, sunny, …]`
- The Markov property means the probability of rain tomorrow depends only on today's weather, not previous days.
  - This is a simplification and not fully realistic since weather depends on many factors (season, geography, solar activity).
  - To capture more dependencies, the state space can be extended (e.g., include season with weather states).

---

# MP - Transition matrix

- An $N × N$ matrix where $N$ = number of states.
- Each entry $(i, j)$ represents the probability of transitioning from state $i$ to state $j$.
- Example matrix for weather:

|       | Sunny | Rainy |
| :---- | :---- | :---- |
| Sunny | 0.8   | 0.2   |
| Rainy | 0.1   | 0.9   |

---

![width:700px](img/paste-1752314557340.png)
