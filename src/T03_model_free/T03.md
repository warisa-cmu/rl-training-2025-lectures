---
marp: true
theme: default
class:
  - invert
author: Nirand Pisutha-Arnond
paginate: true
footer: "RL Training 2025"
math: mathjax
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Prompt:ital,wght@0,100;0,300;0,400;0,700;1,100;1,300;1,400;1,700&display=swap');

    :root {
    font-family: Prompt;
    --hl-color: #D57E7E;
}
h1 {
  font-family: Prompt
}
</style>

# Reinforcement Learning Training 2025

---

# Model-Free Approach

---

# Motivation

Recall in policy iteration
![width:800px](img/paste-1753323392854.png)

- To make this work, we need to know the model dynamics or $p(s^\prime, r | s, a)$.
- However, we do now know $p$.
- Instead, we will resort to _sampling_.
  - Collecting experience by following some policy in the real world or running the agent through a policy in simulation.

---

# Model-Free Learning

- Monte Carlo (MC) methods
- Temporal difference (TD) methods

---

# Monte Carlo

- We use the law of large numbers (LLN) from statistics.
  > - Average of samples is a good estimate for the actual unknown quantity.
  > - This estimate becomes better and better as the number of trials of the experiment (samples) increases.

---

# Monte Carlo

- Re call that We want to calculate
  $$v_{\pi}(s) = \mathtt{E}_{\pi}[G_t|S_t = s]$$
- We let the agent start from this state $S_t = s$, follow the policy $\pi$ to take actions, and keep doing so until termination.
  - We call one round of actions an **episode**.
- We record the total sum of rewards for each episode.
- We average the rewards to get an estimate of $v_{\pi}(s)$ for the policy $\pi$.

> MC methods replaces expected returns with the average of sample returns.

---

# Worked Example

![alt text](img/paste-1753612809653.png)

---

# Solution $v$

![bg contain right:60%](img/paste-1753670347902.png)

---

# Solution $q$

![bg contain right:60%](img/paste-1753628056968.png)

---

![alt text](img/paste-1753670319488.png)

---

# Estimate $v(A)$

- We simulate many episodes.

| Episode | Path      | Reward from $A$     |
| :------ | :-------- | :------------------ |
| 1       | A → C     | $G_{1}$ = 1         |
| 2       | A → B → C | $G_{2}$ = 0 + 2 = 2 |
| 3       | A → B → C | $G_{3}$ = 0 + 2 = 2 |
| 4       | A → C     | $G_{4}$ = 1         |
| ...     | ...       | $G_{n}$             |

---

# Results

Monte Carlo estimates the value function $v(A)$ as the average return observed after visiting A.

$$
v(A) = \frac{G_{1} + G_{2} + G_{3} + G_{4} + ...}{n} = \frac{1 + 2 + 2 + 1 + ...}{n} \rightarrow \frac{4}{3}
$$

---

# Online method

- Instead of averaging all the returns at the end (the sample mean), we can use the incremental (update) method to estimate $v(A)$ as each new return is observed.
- This is also called the "sample-average" update and is given by:

$$v_{n+1} = v_n + \frac{1}{n} (G_n - v_n)$$

---

# Estimate $q(a_1)$ and $q(a_2)$

| Episode | Path      | Actions at $A$ | Reward from Action at $A$ |
| :------ | :-------- | :------------- | :------------------------ |
| 1       | A → C     | $a_2$          | $G_{1} = 1$               |
| 2       | A → B → C | $a_1$          | $G_{2} = 0 + 2$           |
| 3       | A → B → C | $a_1$          | $G_{3} = 0 + 2$           |
| 4       | A → C     | $a_2$          | $G_{4} = 1$               |

---
