---
marp: true
theme: default
class:
  - invert
author: Nirand Pisutha-Arnond
paginate: true
footer: "RL Training 2025"
math: mathjax
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Prompt:ital,wght@0,100;0,300;0,400;0,700;1,100;1,300;1,400;1,700&display=swap');

    :root {
    font-family: Prompt;
    --hl-color: #D57E7E;
}
h1 {
  font-family: Prompt
}
</style>

# Reinforcement Learning Training 2025

---

# Bellman's Equation

> Foundation to solving MDP and RL problems.

---

# Recall (1)

- Markov decision process has transition probabilities

$$\mathtt{Pr}\Big[S_{t+1}=s^\prime, R_{t+1}=r \Big| S_t =s, A_t = a\Big]$$

which transitions the agen to state $S_{t+1}$ and a reward of $R_{t+1}$

- Cumulative reward at time $t$

  $$
  G_t
  = R_{t+1} + \gamma R_{t+2} + ...
  = \sum_{k=0}^{\infty} \gamma^k R_{t+1+k}
  $$

---

# Recall (2)

- A value function is an expected cumulative reward

$$v_{\pi}(s) = \mathtt{E}_{\pi}[G_t|S_t = s]$$

- A action-value value function is an expected cumulative reward from taking action $a$
  $$q_{\pi}(s,a) = E_\pi[G_t | S_t=s, A_t = a]$$

> Note that $v$ and $q$ depend on the policy $\pi$.

---

# Bellman Equation

- Allows relationships among $v$ and $q$.

![alt text](img/paste-1752481072879.png)

![alt text](img/paste-1752481102644.png)

---

# Example

![bg contain right:70%](img/paste-1752559433168.png)

---

# Find $v(s_1)$

![alt text](img/paste-1752559605359.png)

---

# Find $v(s_2)$ and $v(s_3)$

![alt text](img/paste-1752559705628.png)

---

# Find $v(s_0)$

![alt text](img/paste-1752559765769.png)

---

# Result

See values of $v$
![bg contain right:70%](img/paste-1752559865014.png)

---

# Find $q(a_3)$

![alt text](img/paste-1752559934819.png)

---

# Find $q(a_4) - q(a_8)$

![alt text](img/paste-1752559971441.png)

---

# Result

See $q$
![bg contain right:70%](img/paste-1752560100836.png)

---

# $ Find $q(a_1), q(a_2)$

![alt text](img/paste-1752560002276.png)

---

# Result

![bg contain right:70%](img/paste-1752560129581.png)

---

## Can we do better?

![bg contain right:70%](img/paste-1752560129581.png)

---

# Optimal Policy

![bg contain right:70%](img/paste-1752560224613.png)

---

# Optimality Condition

- If agent is following the optimal policy $\pi^*$ (something we want to find), then the value function will also be optimal.

$$v^* (s) = \max_{\pi} v_{\pi}(s)$$

- It follows that

$$v^*(s) = \max_a \Big\{ \sum_{s^\prime, r} p(s^\prime ,r | s, a)[r + \gamma v^*(s^\prime)]\Big\}$$

$$q^*(s,a) =\sum_{s^\prime, r} p(s^\prime ,r | s, a) \cdot \Big[ r    + \gamma \max_{a^\prime} q^*(s^\prime, a^\prime)] \Big]$$

---

# Gridworld Environment

![width:800px](img/paste-1753324256997.png)

---

# Gridworld

- The top-left and bottom-right positions are terminal states, depicted as shaded cells.

- In each cell, the agent can move UP, RIGHT, DOWN, or LEFT.

- Actions deterministically move the agent in the specified direction, unless blocked by a wall.

- If the agent tries to move into a wall, it remains in its current position.

- The agent receives a reward of -1 for each step taken until it reaches a terminal state.

---

# Policy Evaluation / Prediction

From Bellman's equation.
![width:800px](img/paste-1753323380397.png)

We use iterative solution.
![width:800px](img/paste-1753323392854.png)

---

![width:850px](img/paste-1753323508500.png)

---

# Result

- See a python code that uses a random policy.

![bg contain right:50%](./img/paste-1753323748693.png)

---

# Validate Result

![width:850px](img/paste-1753324631217.png)

---

# Policy Improvement

![width:800px](img/paste-1753325832794.png)

---

# Policy Improvement

- You can improve the current policy by choose the action with maximum $q$ value and define this to be the new policy.

![width:400px](img/paste-1753326017285.png)

---

# Policy Improvement

![bg contain right:50%](img/paste-1753326703977.png)

- This is known as the _greedy_ policy.
- Also Note the new state value $v=-1$.

---

# Policy Iteration

![alt text](img/paste-1753326363037.png)

---

![width:650px](img/paste-1753326394929.png)

---

![alt text](img/paste-1753326621302.png)

---

![alt text](img/paste-1753326725488.png)

---

# Value Iteration

- Merge the policy evaluation and policy improvement together into single operation.

![alt text](img/paste-1753327357882.png)

---

# Value Iteration

- The state values will keep improving.
  ![alt text](img/paste-1753327431124.png)

- Once the values converges, we can find the optimum policy.
  ![alt text](img/paste-1753327439091.png)

---

![width:500px](img/paste-1753327484694.png)

---

# Generalized Policy Iteration

- Main idea
  ![alt text](img/paste-1753327840159.png)

---

# Generalized Policy Iteration

- Full convergence + change policy for all states.

  ![alt text](img/paste-1753327658432.png)

---

# Generalized Policy Iteration

- **Partial** convergence + change policy for all states.

![alt text](img/paste-1753327686456.png)

---

# Generalized Policy Iteration

- Full convergence + change policy for **partial** states.

![alt text](img/paste-1753327762993.png)
