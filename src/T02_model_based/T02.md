---
marp: true
theme: default
class:
  - invert
author: Nirand Pisutha-Arnond
paginate: true
footer: "RL Training 2025"
math: mathjax
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Prompt:ital,wght@0,100;0,300;0,400;0,700;1,100;1,300;1,400;1,700&display=swap');

    :root {
    font-family: Prompt;
    --hl-color: #D57E7E;
}
h1 {
  font-family: Prompt
}
</style>

# Reinforcement Learning Training 2025

---

# Bellman's Equation

> Foundation to solving MDP and RL problems.

---

# Recall (1)

- Markov decision process has transition probabilities

$$\mathtt{Pr}\Big[S_{t+1}=s^\prime, R_{t+1}=r \Big| S_t =s, A_t = a\Big]$$

which transitions the agen to state $S_{t+1}$ and a reward of $R_{t+1}$

- Cumulative reward at time $t$

  $$
  G_t
  = R_{t+1} + \gamma R_{t+2} + ...
  = \sum_{k=0}^{\infty} \gamma^k R_{t+1+k}
  $$

---

# Recall (2)

- A value function is an expected cumulative reward

$$v_{\pi}(s) = \mathtt{E}_{\pi}[G_t|S_t = s]$$

- A action-value value function is an expected cumulative reward from taking action $a$
  $$q_{\pi}(s,a) = E_\pi[G_t | S_t=s, A_t = a]$$

> Note that $v$ and $q$ depend on the policy $\pi$.

---

# Bellman Equation

- Allows relationships among $v$ and $q$.

![alt text](img/paste-1752481072879.png)

![alt text](img/paste-1752481102644.png)

---

# Example

![bg contain right:70%](img/paste-1752559433168.png)

---

# Find $v(s_1)$

![alt text](img/paste-1752559605359.png)

---

# Find $v(s_2)$ and $v(s_3)$

![alt text](img/paste-1752559705628.png)

---

# Find $v(s_0)$

![alt text](img/paste-1752559765769.png)

---

# Result

See values of $v$
![bg contain right:70%](img/paste-1752559865014.png)

---

# Find $q(a_3)$

![alt text](img/paste-1752559934819.png)

---

# Find $q(a_4) - q(a_8)$

![alt text](img/paste-1752559971441.png)

---

# Result

See $q$
![bg contain right:70%](img/paste-1752560100836.png)

---

# $ Find $q(a_1), q(a_2)$

![alt text](img/paste-1752560002276.png)

---

# Result

![bg contain right:70%](img/paste-1752560129581.png)

---

## Can we do better?

![bg contain right:70%](img/paste-1752560129581.png)

---

# Optimal Policy

![bg contain right:70%](img/paste-1752560224613.png)

---

# Optimality Condition

- If agent is following the optimal policy $\pi^*$ (something we want to find), then the value function will also be optimal.

$$v^* (s) = \max_{\pi} v_{\pi}(s)$$

- It follows that

$$v^*(s) = \max_a \Big\{ \sum_{s^\prime, r} p(s^\prime ,r | s, a)[r + \gamma v^*(s^\prime)]\Big\}$$

$$q^*(s,a) =\sum_{s^\prime, r} p(s^\prime ,r | s, a) \cdot \Big[ r    + \gamma \max_{a^\prime} q^*(s^\prime, a^\prime)] \Big]$$
