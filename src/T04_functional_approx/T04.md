---
marp: true
theme: default
class:
  - invert
author: Nirand Pisutha-Arnond
paginate: true
footer: "RL Training 2025"
math: mathjax
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Prompt:ital,wght@0,100;0,300;0,400;0,700;1,100;1,300;1,400;1,700&display=swap');

    :root {
    font-family: Prompt;
    --hl-color: #D57E7E;
}
h1 {
  font-family: Prompt
}
</style>

# Reinforcement Learning Training 2025

---

# Functional Approximation

---

# Motivation

- Up till now, we only deal with problems with **discrete** state space.
  - $v$ and $q$ are represented in a table.
  - _Tabular_ case
- Real problem has too many states.
  - The tabular approach requires too much memory and computation.

---

# Formulation

- Instead of representing values in a table, they are now being represented by
  ![alt text](img/paste-1756084789805.png)

- where the parameter $w$ is the parameter of the function that defines the policy $\pi(a|s)$ that the agent follows.

---

# What is $w$?

![alt text](img/paste-1756084789805.png)

- Tunable parameters of the function that defines the policy.
- Weights of a deep learning neural network.

---

# Changing $w$

- When you update the weight vector $w$ based on some update equation for a specific state $s$ or state action pair $(s, a)$
  - it not only updates the $v$ or $q$ for that specific $s$ or $(s, a)$
  - but it also updates other _nearby_ $v$ and $q$.
- This is different from the tabular case where you can update each $v$ or $q$ independently.

---

# Functional approach

![alt text](img/paste-1756085243912.png)

---

# How do we find update equation for $w$?

> We used technique from supervised learning.

- `1` Define a loss function
  ![alt text](img/paste-1756085865474.png)
- `2` Update $w$ using gradient descent.
  ![alt text](img/paste-1756085919199.png)

---

# Monte Carlo

- Using the update equation (from before)
  ![alt text](img/paste-1756086286511.png)
- Gives
  ![alt text](img/paste-1756086381395.png)

---

# Temporal Difference (TD(0))

- Using the update equation (from before)
  ![alt text](img/paste-1756086349575.png)
- Gives
  ![alt text](img/paste-1756086439635.png)

---

# Linear Model

- State value function.
  ![alt text](img/paste-1756086879793.png)
- Gradient
  ![alt text](img/paste-1756087547581.png)

---

# Linear Model

- Update equation
  ![alt text](img/paste-1756087553228.png)

- In MC, $V_{\pi} = g_t$
- In TD(0), $V_{\pi} = R_{t+1} + \gamma \cdot v_t(s^\prime)$

---

# Linear Model

- Update equations
  ![alt text](img/paste-1756087738590.png)

---

# Challenge

- Recalled the loss function.
  ![width:400px](img/paste-1756086561915.png)
- In supervised learning, $y(t)$ are labels that do not change. _(No problem)_

- In RL, $y(t)$ are $v$ or $q$ which keep changing (because we are finding them)
  - _Nonstationarity_ problem
- Also, in TD(0), we use bootstrapping (target is not a true value).
  - This is even worse.

---

![alt text](img/paste-1756087864232.png)

---

# Control

- Similar to how you find $v$, here we use $q$
  ![alt text](img/paste-1756087922731.png)
- Cost function
  ![alt text](img/paste-1756087975384.png)
- Update equation
  ![alt text](img/paste-1756087956431.png)

---

# Control

- Gives
  ![alt text](img/paste-1756088291729.png)

---

# Control

- We use GPI (same as before).
  ![alt text](img/paste-1756088339456.png)

---

# Semi Gradient $n$-Step SARSA

- On policy
  - $\epsilon$-greedy policy for evaluation and control.

---

# Tile Encoding

![alt text](img/paste-1756101064902.png)

---

# Tile Encoding

- Binary features

  ![alt text](img/paste-1756101088608.png)
  ![alt text](img/paste-1756101095354.png)

---

![alt text](img/paste-1756099881752.png)

---

![width:650px](img/paste-1756099997550.png)

---

# Semi-gradient SARSA($\lambda$)

- _Skip for now_

---

# Instability

> 3 ways that can cause instability

![alt text](img/paste-1756101449396.png)

---

![alt text](img/paste-1756101510998.png)
